# Question-Answering-System
QA system employing DistilBERT

What is DisttilBERT?

It is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased , runs 60% faster while preserving over 95% of BERT's performances.
